{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "de373726",
   "metadata": {},
   "source": [
    "# Martingales"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "c1df6b6c",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1f8f5386",
   "metadata": {},
   "source": [
    "Throughout the exercises we will use our standard notion for hitting times. $T_a =min\\{n\\geq 1:X_n =a\\}$ and $V_a =min\\{n\\geq 0:X_n =a\\}$."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2cc88a6c",
   "metadata": {},
   "source": [
    "**5.1. _Brother–sister mating_. Consider the six state chain defined in Exercise 1.66. Show that the total number of A’s is a martingale and use this to compute the probability of getting absorbed into the 2,2 (i.e., all A’s state) starting from each initial state.**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "17f73feb",
   "metadata": {},
   "source": [
    "```\n",
    "    22  21  20  11  10  00\n",
    "22  1   0   0   0   0   0\n",
    "21 1/4 1/2  0  1/4  0   0 \n",
    "20  0   0   0   1   0   0 \n",
    "11 1/16 1/4 1/8 1/4 1/4 1/16 \n",
    "10  0   0   0  1/4 1/2 1/4\n",
    "00  0   0   0   0   0   1\n",
    "```\n",
    "\n",
    "Let $X_t$ be $i+j$ or number of A's in the code and $Y_t$ be the code itself at time $t$. $E[X_{t+1}| Y_t]=X_t$:\n",
    " - $E[X_{t+1}| Y_t=22]=4$\n",
    " - $E[X_{t+1}| Y_t=21]=\\frac{1}{4}4 +\\frac{1}{2}3 + \\frac{1}{4}2=3$\n",
    " - $E[X_{t+1}| Y_t=20]=2$\n",
    " - $E[X_{t+1}| Y_t=11]=\\frac{1}{16}4 +\\frac{1}{4}3 + \\frac{1}{8}2 +\\frac{1}{4}2 + \\frac{1}{4}1=2$ \n",
    " - $E[X_{t+1}| Y_t=10]=\\frac{1}{4}2 +\\frac{1}{2}1 + \\frac{1}{4}0=1$\n",
    " - $E[X_{t+1}| Y_t=00]=0$\n",
    " \n",
    "It is a martingale, in order to use theorem 5.11 we need to define and check stopping time $T=min\\{n\\geq: Y_n \\in \\{22, 00\\}\\}$, from the transition matrix we know $P(T<\\infty)=1$. Now we know $E_{X_0}[X_t]=E[X_0]=X_0$, therefore, using Theorem 5.11 $P_{X_0}(V_{22}<V_{00})=\\frac{X_0}{4}$."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "427a6b04",
   "metadata": {},
   "source": [
    "**5.2. Lognormal stock prices. Consider the special case of Example 5.4 in which $X_i = e^{\\eta_i}$ where $\\eta_i = normal(\\mu, \\sigma^2)$. For what values of $\\mu$ and $\\sigma$ is $M_n = M_0 X_1 \\dots X_n$ a martingale?**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ec9c7bac",
   "metadata": {},
   "source": [
    "We want to show that $E(M_{n+1}-M_n|F_n)=0$.\n",
    "$$E(M_{n+1}-M_n|F_n)=M_nE(X_{n+1}-1)=M_n\\big(e^{E{\\eta_{n+1}}}-1\\big)$$\n",
    "$e^{E{\\eta_{n+1}}}=e^{\\mu+\\sigma^2/2}$, see [wiki](https://en.wikipedia.org/wiki/Log-normal_distribution), $e^{\\mu+\\sigma^2/2} = 1$ iff $\\mu+\\frac{\\sigma^2}{2}=0$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7e526da1",
   "metadata": {},
   "source": [
    "**5.3. Let $X_n$ be the _Wright–Fisher_ model with no mutation defined in Example 1.9. (a) Show that $X_n$ is a martingale and use Theorem 5.11 to conclude that $P_x(V_N < V_0)=x/N$. (b) Show that $Y_n = X_n(N-X_n)/(1-1/N)^n$ is a martingale. (c) Use this to conclude that**\n",
    "$$(N-1)\\leq\\frac{x(N − x)(1 − 1/N)^n}{P_x(0<X_n<N)}\\leq\\frac{N^2}{4}$$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "04e5b557",
   "metadata": {},
   "source": [
    "(a) $X_n$ follow the Markov chain with $p(i, j)=\\binom{N}{j}\\frac{1}{N}^j(1-\\frac{1}{N})^{N-j}$ we can clearly see that it a [Binomial theorem](https://en.wikipedia.org/wiki/Binomial_theorem), $\\sum_{j=0}^{N}p(i,j)=(\\frac{j}{N}+1-\\frac{j}{N})^N=1$  \n",
    "(b) Let $X_0=x$, then $X_1\\sim Binomial(\\frac{x}{N}, N)$, therefore,  \n",
    "$E_x(X_1)=\\frac{x}{N}N=x$ and $E_x(X_1^2)=N\\frac{x}{N}(1-\\frac{x}{N}) + x^2=x(1-\\frac{x}{N}) + x^2$.  \n",
    "Now let $E_x(X_1(N-X_1))=xN - x(1-\\frac{x}{N}) - x^2=x(1-\\frac{x}{N})(N-x)$. similarly we could get $E_x(X_n(N-X_n))=x(1-\\frac{x}{N})(N-x)^n$. To prove $Y_n$ is a martingale, we need to show: $E(Y_{n+1}-Y_n|F_n)=0$.\n",
    "$$E(Y_{n+1}-Y_n|F_n)=\\frac{X_{n+1}(N-X_{n+1})}{(1-1/N)^{n+1}}-\\frac{X_n(N-X_n)}{(1-1/N)^n}=\\frac{X_{n+1}(N-X_{n+1}) - X_n(N-X_n)(1-1/N)}{(1-1/N)^{n+1}}=\\frac{\\hat{E_x}(X_{n+1}(N-X_{n+1})) - \\hat{E_x}(X_n(N-X_n))(1-1/N)}{(1-1/N)^{n+1}}$$\n",
    "\n",
    "(c) $P_x(0<X_n<N)=((1-1/N)^n)$, when $0 \\le X_n \\le N$ then $min_n\\{X_n(N-X_n)\\}=(N-1)(N-(N-1))=1(N-1)=N-1$ and $max_n\\{X_n(N-X_n)\\}=\\frac{N}{2}(N-\\frac{N}{2})=\\frac{N^2}{4}$, therefore,\n",
    "$$(N-1)\\leq\\frac{x(N − x)(1 − 1/N)^n}{P_x(0<X_n<N)}\\leq\\frac{N^2}{4}$$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a814736b",
   "metadata": {},
   "source": [
    "**5.4. _An unfair fair game_. Define random variables recursively by $Y_0=1$ and for $n \\geq 1$, $Y_n$ is chosen uniformly on $(0, Y_{n−1})$. If we let $U_1, U_2, \\dots$ be uniform on $(0, 1)$, then we can write this sequence as $Y_n = U_nU_{n−1}\\dots U_0$. (a) Use Example 5.4 to conclude that $M_n = 2^nY_n$ is a martingale. (b) Use the fact that $logY_n=logU_1+\\dots+logU_n$ to show that $(1/n)logX_n \\rightarrow −1$. (c) Use (b) to conclude $M_n \\rightarrow 0$, i.e., in this \"fair\" game our fortune always converges to 0 as time tends to $\\infty$.**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "424d62fc",
   "metadata": {},
   "source": [
    "(a) $E\\big[M_{n+1}|F_n\\big]=2^{n+1}Y_nE\\big[U_{n+}\\big]=2^{n+1}Y_n\\frac{1}{2}=2^nY_n$  \n",
    "(b) $P(-logU_i>x)=P(logU_i<-x)=P(U_i<e^-x)=e^-x$ with mean 1. With the law of large numbers $\\frac{logU_1+\\dots+logU_n}{n} \\rightarrow -1$  \n",
    "(c) $\\frac{-logM_n}{n}=\\frac{-1 + log2}{n}>0$ therefore $M_n \\rightarrow \\infty$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "db65653f",
   "metadata": {},
   "source": [
    "**5.5. Consider a favorable game in which the payoffs are −1, 1, or 2 with probability 1/3 each. Use the results of Example 5.17 to compute the probability we ever go broke (i.e, our winnings $W_n$ reach \\\\$0) when we start with $\\$i$.**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e3c0404f",
   "metadata": {},
   "source": [
    "From Example 5.17 and Theorem 5.13 we know that:\n",
    " - $V_0 = min\\{n:S_n=0\\}$\n",
    " - $\\Phi(\\theta)=e^{\\theta X_i}$ where $\\Phi(\\alpha)=1$\n",
    " - $P_x(V_0<\\infty) = e^{\\alpha x}$\n",
    " \n",
    "$$\\Phi(\\theta)=\\frac{e^{-\\theta}+e^{\\theta}+e^{2\\theta}}{3}=1$$\n",
    "$$3e^{\\theta}=1+e^{2\\theta}+e^{3\\theta},\\ s = e^{\\theta}$$\n",
    "$$s^3+s^2-3s+1=0,\\ (s_1=1)\\ is\\ one\\ root$$\n",
    "$$s^2+2x-1=0$$\n",
    "$$s_2=\\frac{-2+2\\sqrt{2}}{2}\\ s_3=\\frac{-2-2\\sqrt{2}}{2},\\ \\alpha=\\sqrt{2}-1$$\n",
    "$$P_x(V_0<\\infty)=e^{\\alpha x}=e^{(\\sqrt{2}-1)x}$$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7ed49cc9",
   "metadata": {},
   "source": [
    "**5.6. Let $S_n = X_1+\\dots+X_n$ where the $X_i$ are independent with $EX_i=0$ and $var(X_i)=\\sigma^2$. (a) Show that $S_n^2 - n\\sigma^2$ is a martingale. (b) Let $\\tau=min\\{n : |S_n| > a\\}$. Use Theorem 5.10 to show that $E\\tau \\geq a^2/\\sigma^2$. For simple random walk $\\sigma^2 = 1$ and we have equality.**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8c7a8536",
   "metadata": {},
   "source": [
    "(a) Using the Example 5.3 $M_{n+1}-M_n = (S_n+X_{n+1})^2-S_n -\\sigma^2=S_n^2 + 2S_nX_{n+1} +X_{n+1}^2-S_n -\\sigma^2$\n",
    "$$E(S_n^2 + 2S_nX_{n+1} +X_{n+1}^2-S_n^2 -\\sigma^2)=S_n^2-n\\sigma^2$$\n",
    " - $E(S_nX_{n+1})=0$\n",
    " - $E(S_n^2)=n\\sigma^2$\n",
    " - $E(X_{i}^2)=\\sigma^2$\n",
    " \n",
    "(b) By Theorem 5.10 we have $\\sigma^2E(\\tau\\land n)=E(S^2_{\\tau\\land n})\\geq a^2$. For a large $n \\rightarrow \\infty$ we get $E(\\tau)\\geq \\frac{a^2}{\\sigma^2}$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6cc4c01d",
   "metadata": {},
   "source": [
    "**5.7. _Wald's second equation_. Let $S_n = X_1 +\\dots+X_n$ where the $X_i$ are independent with $EX_i = 0$ and $var(X_i) = \\sigma^2$. Use the martingale from the previous problem to show that if $\\tau$ is a stopping time with $E\\tau < \\infty$ then $ES_\\tau^2 = \\sigma^2E\\tau.$**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b0146df9",
   "metadata": {},
   "source": [
    "From the previous example and the Theorem 5.10, we get:\n",
    "$$E(M_{\\tau\\land n})=E(M_0)$$\n",
    "$$E(S^2_{\\tau\\land n}) - \\sigma^2 E(\\tau\\land n)=E(S^2_0)=0$$\n",
    "$$E(S^2_{\\tau\\land n})=\\sigma^2 E(\\tau\\land n),\\ n \\rightarrow \\infty,\\ E(S^2_{\\tau})=\\sigma^2 E(\\tau)$$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6a28a346",
   "metadata": {},
   "source": [
    "**5.8. _Variance of the time of gambler's ruin_. Let $\\xi_1, \\xi_2,\\dots$ be independent with $P(\\xi_i=1)=p$ and $P(\\xi_i=-1)=q=1−p$ where $p<1/2$. Let $S_n = S_0 +\\xi_1 +\\dots+\\xi_n$. In Example 4.3 we showed that if $V_0 = min\\{n \\leq 0 : S_n = 0\\}$ then $E_xV_0 = x/(1 - 2p)$. The aim of this problem is to compute the variance of $V_0$. (a) Show that $(S_n -(p-q)n)^2-n(1-(p-q)^2)$ is a martingale. (b) Use this to conclude that when $S_0=x$ the variance of $V_0$ is**\n",
    "$$x\\frac{1-(p-q)^2}{(p-q)^3}$$\n",
    "**(c) Why must the answer in (b) be of the form $cx$?**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6cee7900",
   "metadata": {},
   "source": [
    "(a) $S^2_n -n\\sigma^2$ is a martingale in this case $var(\\xi_i)=p+q-(p+q)^2=1-(p+q)^2$ and $E(\\xi_i)=p-q$ we can see that $(S_n -(p-q)n)^2-n(1-(p-q)^2)$ is a martingale.\n",
    "(b)  \n",
    " - $\\big[S^2_n-(p-q)n\\big]^2=S_n^2-2(p-q)n S_n+(p-q)^2n^2$\n",
    " - realising that $S_{V_0}=0$ and $S^2_{V_0}=1-(p-q)^2E_x[V_0]$  \n",
    " - $(p-q)^2 E[{V_0}^2]-1-(p-q)^2E_x[V_0]=x^2$\n",
    " - $E[{V_0}^2]=x\\frac{1-(p-q)^2}{(p-q)^3} + \\frac{x^2}{(p-q)^2}$\n",
    " - $var(V_0)=E[{V_0}^2]-E[{V_0}]^2=x\\frac{1-(p-q)^2}{(p-q)^3}$\n",
    " \n",
    "(c) From [Strong Markov property](https://en.wikipedia.org/wiki/Markov_property#Strong_Markov_property) we know that each of the $V_y$ are independent and has to be have the same distribution."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "65a54858",
   "metadata": {},
   "source": [
    "**5.9. _Generating function of the time of gambler's ruin_. Continue with the set-up of the previous problem. (a) Use the exponential martingale and our stopping theorem to conclude that if $\\theta \\leq 0$, then $e^{\\theta x} = E_x(\\Phi(\\theta)^{-V_0})$. (b) Let $0<s<1$. Solve the equation $\\Phi(\\theta)=1/s$, then use (a) to conclude**\n",
    "$$E_x(s^{V_0})=\\bigg(\\frac{1-\\sqrt{1-4pqs^2}}{2ps}\\bigg)^x$$\n",
    "**(c) Why must the answer in (b) be of the form $f(s)^x$?**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "297d6420",
   "metadata": {},
   "source": [
    "(c) From [Strong Markov property](https://en.wikipedia.org/wiki/Markov_property#Strong_Markov_property) we know that each of the $V_y$ are independent and has to be have the same distribution. Sum of exponential i.i.d variables is its power. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f875ab6d",
   "metadata": {},
   "source": [
    "**5.10. Let $Z_n$ be a branching process with offspring distribution $p_k$ with $p_0 > 0$ and $\\mu = \\sum_k kp_k > 1$. Let $\\Phi(\\theta) = \\sum^\\infty_{k=0} p_k\\theta^k$. (a) Show that $E(\\theta^{Z_{n+1}} |Z_n) = \\Phi(\\theta)^{Z_n}$. (b) Let $\\rho$ be the solution $< 1$ of $\\Phi(\\rho) = \\rho$ and conclude that $P_k(T_0 < \\infty) = \\rho^k$**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "96f4ce87",
   "metadata": {},
   "source": [
    "(a) Using the [property of generating function](https://en.wikipedia.org/wiki/Probability-generating_function#Univariate_case) $\\Phi(\\theta)=E[\\theta^{Y_n}]=\\sum^\\infty_{k=0} p_k\\theta^k$, we can see\n",
    "$$E(\\theta^{Z_{n+1}} |Z_n)=E(\\theta^{Z_{n}+Y_{n+1}}|Z_n)=\\Phi(\\theta)^{Z_n}$$\n",
    "(b) Let $T_0$ be a stopping time when $Z_n=0$  \n",
    "$$\\rho^k=\\Phi(\\rho)^k=E(\\rho^{Z_{T_0 \\land n}}|Z_{T_0 \\land n}=k)=P_k(T_0<n)+E(\\rho^{Z_{n}}|T_0 > n)$$\n",
    "Since $p_0>0$ then the process will vist the 0 infinitely many times, so $n\\rightarrow \\infty,\\ \\ E(\\rho^{Z_{n}}|T_0 > n)\\rightarrow 0 $"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
